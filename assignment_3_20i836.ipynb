{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Replace 'your_file.csv' with the name of your CSV file\n",
        "file_path = '/content/drive/My Drive/datamining/your_file.csv'\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2z37VXcTgck",
        "outputId": "fc4bce6a-9715-4d15-a412-c4902643aef5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "cxHotcrmTOEo"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import random\n",
        "\n",
        "# Load data\n",
        "train_df = pd.read_csv('/content/drive/My Drive/datamining/train.csv')\n",
        "test_df = pd.read_csv('/content/drive/My Drive/datamining/test.csv')\n",
        "test_label_df = pd.read_csv('/content/drive/My Drive/datamining/test_label.csv')\n",
        "\n",
        "# Normalize the features\n",
        "scaler = StandardScaler()\n",
        "train_scaled = scaler.fit_transform(train_df)\n",
        "test_scaled = scaler.transform(test_df)\n",
        "\n",
        "# Define a dataset class\n",
        "class TimeSeriesDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = TimeSeriesDataset(train_scaled)\n",
        "test_dataset = TimeSeriesDataset(test_scaled)\n",
        "\n",
        "# DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerAutoencoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
        "        print(input_dim)\n",
        "        super(TransformerAutoencoder, self).__init__()\n",
        "        self.encoder = nn.TransformerEncoderLayer(d_model=input_dim, nhead=4, dim_feedforward=hidden_dim, batch_first=True)\n",
        "        self.decoder = nn.TransformerEncoderLayer(d_model=input_dim, nhead=4, dim_feedforward=hidden_dim, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return decoded\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_dim, 256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(256, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(Generator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Initialize models\n",
        "input_dim = train_scaled.shape[1]\n",
        "hidden_dim = 128\n",
        "autoencoder = TransformerAutoencoder(input_dim, hidden_dim, 3)\n",
        "discriminator = Discriminator(input_dim)\n",
        "generator = Generator(input_dim, input_dim)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "id": "G1UtN1Y6UBb7",
        "outputId": "783bbc74-9018-43fd-cb4d-ba0941179cd3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "26\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "embed_dim must be divisible by num_heads",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-2ff2fb7d16ad>\u001b[0m in \u001b[0;36m<cell line: 41>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0minput_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_scaled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mhidden_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mautoencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTransformerAutoencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0mdiscriminator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-2ff2fb7d16ad>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dim, hidden_dim, num_layers)\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTransformerAutoencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTransformerEncoderLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnhead\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_feedforward\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTransformerEncoderLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnhead\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_feedforward\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, d_model, nhead, dim_feedforward, dropout, activation, layer_norm_eps, batch_first, norm_first, bias, device, dtype)\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0mfactory_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dtype'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m         self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout,\n\u001b[0m\u001b[1;32m    561\u001b[0m                                             \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_first\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m                                             **factory_kwargs)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, embed_dim, num_heads, dropout, bias, add_bias_kv, add_zero_attn, kdim, vdim, batch_first, device, dtype)\u001b[0m\n\u001b[1;32m    989\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_first\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    990\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membed_dim\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 991\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_heads\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"embed_dim must be divisible by num_heads\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    992\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qkv_same_embed_dim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: embed_dim must be divisible by num_heads"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerAutoencoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
        "        super(TransformerAutoencoder, self).__init__()\n",
        "        # Adjust the number of heads to fit your specific input_dim\n",
        "        num_heads = 4 if input_dim % 4 == 0 else 2 if input_dim % 2 == 0 else 1\n",
        "        self.encoder = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=input_dim, nhead=num_heads, dim_feedforward=hidden_dim, batch_first=True), num_layers=num_layers)\n",
        "        self.decoder = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=input_dim, nhead=num_heads, dim_feedforward=hidden_dim, batch_first=True), num_layers=num_layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return decoded\n",
        "\n",
        "# Assuming your input_dim here. Replace it with the actual dimension if different.\n",
        "input_dim = train_scaled.shape[1]\n",
        "hidden_dim = 128\n",
        "autoencoder = TransformerAutoencoder(input_dim, hidden_dim, num_layers=3)\n",
        "discriminator = Discriminator(input_dim)\n",
        "generator = Generator(input_dim, input_dim)\n",
        "\n",
        "# Continue with the rest of the setup and training as previously described\n"
      ],
      "metadata": {
        "id": "LaI2peLzU_X_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ContrastiveLoss(nn.Module):\n",
        "    def __init__(self, margin=1.0):\n",
        "        super(ContrastiveLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "        self.eps = 1e-9\n",
        "\n",
        "    def forward(self, pos, neg):\n",
        "        # Distance between the positive pairs (should be low)\n",
        "        pos_dist = torch.sqrt(torch.sum((pos[0] - pos[1]) ** 2, dim=1) + self.eps)\n",
        "        # Distance between the negative pairs (should be high)\n",
        "        neg_dist = torch.sqrt(torch.sum((neg[0] - neg[1]) ** 2, dim=1) + self.eps)\n",
        "\n",
        "        # Contrastive loss calculation\n",
        "        loss = torch.mean(pos_dist + torch.clamp(self.margin - neg_dist, min=0))\n",
        "        return loss\n"
      ],
      "metadata": {
        "id": "zZytW0fKVK5U"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def augment_data(data, mask_rate=0.2):\n",
        "    mask = np.random.geometric(p=mask_rate, size=data.shape) > 1\n",
        "    return data * mask\n"
      ],
      "metadata": {
        "id": "TRrwLGWdVQCB"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "S81KV27vVUWL",
        "outputId": "d1ef487a-f24b-470d-ee1c-1d8273d77537"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'numpy.ndarray' object has no attribute 'to'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-bce7a0442fab>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mreal_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0maugmented_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maugment_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# --- Train Autoencoder ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'to'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hJEqp426iG9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "gNdNMU96VwYa",
        "outputId": "2faa0f50-3bb2-4cf7-9f15-62975a2f4307"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 must have the same dtype, but got Double and Float",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-a2f093b640aa>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;31m# --- Train Generator ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0moptimizer_g\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mfake_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maugmented_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mfake_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mloss_g\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion_gan_g\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-2ff2fb7d16ad>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m# Initialize models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 must have the same dtype, but got Double and Float"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "A8FbuZd5iIq8",
        "outputId": "809d6ce9-5e81-41ee-e821-06d7cc663e2b"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py:266: UserWarning: Error detected in MseLossBackward0. Traceback of forward call that caused the error:\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "    ColabKernelApp.launch_instance()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-39-2c1cbedd9ac0>\", line 17, in <cell line: 5>\n",
            "    loss_ae = criterion_ae(reconstructed_data.float(), real_data.float())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\", line 535, in forward\n",
            "    return F.mse_loss(input, target, reduction=self.reduction)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 3339, in mse_loss\n",
            "    return torch._C._nn.mse_loss(expanded_input, expanded_target, _Reduction.get_enum(reduction))\n",
            " (Triggered internally at ../torch/csrc/autograd/python_anomaly_mode.cpp:113.)\n",
            "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Function 'MseLossBackward0' returned nan values in its 0th output.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-2c1cbedd9ac0>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mloss_ae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion_ae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreconstructed_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mloss_ae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0moptimizer_ae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             )\n\u001b[0;32m--> 522\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    523\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Function 'MseLossBackward0' returned nan values in its 0th output."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "a45x2IjQlfh9",
        "outputId": "24834495-1fba-4900-b2eb-623a034d5e86"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py:266: UserWarning: Error detected in MseLossBackward0. Traceback of forward call that caused the error:\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "    ColabKernelApp.launch_instance()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-40-8b1651601128>\", line 24, in <cell line: 12>\n",
            "    loss_ae = criterion_ae(reconstructed_data.float(), real_data.float())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\", line 535, in forward\n",
            "    return F.mse_loss(input, target, reduction=self.reduction)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 3339, in mse_loss\n",
            "    return torch._C._nn.mse_loss(expanded_input, expanded_target, _Reduction.get_enum(reduction))\n",
            " (Triggered internally at ../torch/csrc/autograd/python_anomaly_mode.cpp:113.)\n",
            "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Function 'MseLossBackward0' returned nan values in its 0th output.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-8b1651601128>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mloss_ae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion_ae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreconstructed_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mloss_ae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0moptimizer_ae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             )\n\u001b[0;32m--> 522\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    523\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Function 'MseLossBackward0' returned nan values in its 0th output."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YQpX5TDWl3HF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Assuming definitions of your models, optimizers, and data loaders\n",
        "\n",
        "# Enable anomaly detection\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "# Training loop\n",
        "epochs = 50\n",
        "for epoch in range(epochs):\n",
        "    for data in train_loader:\n",
        "        real_data = data.to(device)\n",
        "        augmented_data_np = augment_data(real_data.cpu().numpy())\n",
        "        augmented_data = torch.from_numpy(augmented_data_np).to(device)\n",
        "\n",
        "        # --- Train Autoencoder ---\n",
        "        optimizer_ae.zero_grad()\n",
        "\n",
        "        # Convert reconstructed_data to Float\n",
        "        reconstructed_data = autoencoder(real_data.float())\n",
        "\n",
        "        # Remove NaN values from input and output\n",
        "        mask = ~torch.isnan(reconstructed_data) & ~torch.isnan(real_data)\n",
        "        reconstructed_data = reconstructed_data[mask]\n",
        "        real_data = real_data[mask]\n",
        "\n",
        "        if reconstructed_data.numel() == 0:\n",
        "            print(\"All samples in the batch contain NaN values. Skipping this batch.\")\n",
        "            continue\n",
        "\n",
        "        loss_ae = criterion_ae(reconstructed_data.float(), real_data.float())\n",
        "        loss_ae.backward()\n",
        "        optimizer_ae.step()\n",
        "\n",
        "        # --- Train Discriminator ---\n",
        "        optimizer_d.zero_grad()\n",
        "        real_labels = torch.ones(real_data.size(0), 1).to(device)\n",
        "        fake_labels = torch.zeros(real_data.size(0), 1).to(device)\n",
        "\n",
        "        real_output = discriminator(real_data.float())\n",
        "\n",
        "        # Remove NaN values from discriminator output and labels\n",
        "        mask = ~torch.isnan(real_output) & ~torch.isnan(real_labels)\n",
        "        real_output = real_output[mask]\n",
        "        real_labels = real_labels[mask]\n",
        "\n",
        "        loss_d_real = criterion_gan_d(real_output, real_labels)\n",
        "\n",
        "        fake_data = generator(augmented_data.float()).detach()\n",
        "        fake_output = discriminator(fake_data)\n",
        "\n",
        "        # Remove NaN values from discriminator output and labels\n",
        "        mask = ~torch.isnan(fake_output) & ~torch.isnan(fake_labels)\n",
        "        fake_output = fake_output[mask]\n",
        "        fake_labels = fake_labels[mask]\n",
        "\n",
        "        loss_d_fake = criterion_gan_d(fake_output, fake_labels)\n",
        "\n",
        "        loss_d = (loss_d_real + loss_d_fake) / 2\n",
        "        loss_d.backward()\n",
        "        optimizer_d.step()\n",
        "\n",
        "        # --- Train Generator ---\n",
        "        optimizer_g.zero_grad()\n",
        "        fake_data = generator(augmented_data)\n",
        "        fake_output = discriminator(fake_data)\n",
        "        loss_g = criterion_gan_g(fake_output, real_labels)\n",
        "        loss_g.backward()\n",
        "        optimizer_g.step()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{epochs}], Loss AE: {loss_ae.item()}, Loss D: {loss_d.item()}, Loss G: {loss_g.item()}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgqJvHjEmVaQ",
        "outputId": "2588457a-431e-42af-9676-902b1412851b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n",
            "All samples in the batch contain NaN values. Skipping this batch.\n"
          ]
        }
      ]
    }
  ]
}